{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2da6653-68b0-4df0-b14b-7870ec3070e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nglui import parser, statebuilder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "\n",
    "from caveclient import CAVEclient"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed14f1b3-7c77-4da4-bf8c-76b41277adf5",
   "metadata": {},
   "source": [
    "## Generating a streamline\n",
    "Building a streamline requires only a few parameters and a neuroglancer state.\n",
    "The neuroglancer state should include an annotation layer with linked segmentation ids.\n",
    "For several neurons, lay down points every 50 microns or so along the principle pia-to-white matter axis.\n",
    "The apical dendrite and primary axon of layer 5 thick tufted cells (or ET cells) is a particularly good case, as are tall layer 6 apical dendrites.\n",
    "It's important to have overlapping coverage from several neurons at all depths from layer 2 to white matter.\n",
    "Stop placing points when apical tufts bifurcate, as that no longer represents the \"up\" direction for the neuron.\n",
    "It is important that the layer use linked segmentation ids, so that the collection of points associated with each neuron can be easily separated.\n",
    "\n",
    "### Parameters\n",
    "* `datastack_name` : Caveclient datastack name.\n",
    "* `state_id` : State ID to download the neuroglancer state described above.\n",
    "* `streamline_layer_name` : Name of the annotation layer with the streamline points.\n",
    "* `y_min`, `y_max` : Top and bottom depths through which the final streamline points should be produced. Typically, a bit above the pial surface to within white matter. The solver will linearly extrapolate beyond the actual data, but this will produce complete coverage and be a useful visual helper. Note that this should be in standard neuroglancer voxel units.\n",
    "* `x_ctr`, `z_ctr` : The x, z values to use for the top of the streamline. This should be near the center of the cells you've taken, in case the streamline changes with space in your volume. Note that this should be in standard neuroglancer voxel units.\n",
    "* `density`: Density of points along the y-axis. Note that it should be in standard neuroglancer voxel units.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2101ab0d-70d5-43ba-b694-17ab90eb8840",
   "metadata": {},
   "outputs": [],
   "source": [
    "datastack_name = 'minnie65_phase3_v1'\n",
    "state_id = 5912681645080576\n",
    "streamline_layer_name = 'streamline'\n",
    "\n",
    "# All values below are in neuroglancer voxel coordinates\n",
    "y_min = 85_000\n",
    "y_max = 280_000\n",
    "\n",
    "x_ctr = 181000\n",
    "z_ctr = 21600\n",
    "\n",
    "density = 1_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10df21a-4532-4843-a114-080d49057516",
   "metadata": {},
   "source": [
    "### Running\n",
    "\n",
    "After setting parameters in the next cell, run all cells in this section to produce a streamline based on the average vector differences in x and z along the depth axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1cf20d-9fe9-48ee-b984-b0b2054ff5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = CAVEclient(datastack_name)\n",
    "\n",
    "state = client.state.get_state_json(state_id)\n",
    "\n",
    "# Read in the neuroglancer state.\n",
    "\n",
    "anno_df = parser.annotation_dataframe(state)\n",
    "sl_df = anno_df.query('layer == @streamline_layer_name').reset_index(drop=True)\n",
    "\n",
    "group = sl_df['linked_segmentation'].apply(lambda x: x[0])\n",
    "points = np.vstack(sl_df['point'].values)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'pt_x' : points[:,0],\n",
    "        'pt_y' : points[:,1],\n",
    "        'pt_z' : points[:,2],\n",
    "        'group' : group,\n",
    "    }\n",
    ")\n",
    "\n",
    "grp_id = np.unique(df['group'])\n",
    "grp_remap = {gid: ii for ii, gid in enumerate(grp_id)}\n",
    "df['group'] = df['group'].apply(lambda x: grp_remap[x])\n",
    "\n",
    "# Produce interpolated datapoints based on the real data in order to match depths.\n",
    "\n",
    "all_new_dx = []\n",
    "all_new_y = []\n",
    "all_new_dz = []\n",
    "all_new_groups = []\n",
    "for ii in np.unique(df['group']):\n",
    "    df_g = df.query('group == @ii').sort_values(by='pt_y')\n",
    "    fx_g = interpolate.interp1d(df_g['pt_y'], df_g['pt_x'], kind='linear', bounds_error=False)\n",
    "    fz_g = interpolate.interp1d(df_g['pt_y'], df_g['pt_z'], kind='linear', bounds_error=False)\n",
    "\n",
    "    new_y = np.arange(y_min, y_max, density)\n",
    "    new_x = fx_g(new_y)\n",
    "    new_z = fz_g(new_y)\n",
    "    del_x = np.diff(new_x)\n",
    "    del_y = np.diff(new_y)\n",
    "    del_z = np.diff(new_z)\n",
    "    \n",
    "    all_new_dx.append(del_x)\n",
    "    all_new_y.append(new_y[:-1])\n",
    "    all_new_dz.append(del_z)\n",
    "    all_new_groups.append([ii]*(len(new_y)-1))\n",
    "\n",
    "# Average the vector difference at each depth value and then generate a dense path \n",
    "\n",
    "dfi = pd.DataFrame(\n",
    "    {\n",
    "    'del_x': np.concatenate(all_new_dx),\n",
    "    'del_z': np.concatenate(all_new_dz),\n",
    "    'pt_y': np.concatenate(all_new_y),\n",
    "    'group': np.concatenate(all_new_groups),\n",
    "    }\n",
    ").dropna()\n",
    "\n",
    "dfi_avg = dfi.groupby('pt_y').mean()\n",
    "\n",
    "all_pts = np.vstack(\n",
    "    [x_ctr+np.cumsum(dfi_avg['del_x']),\n",
    "     dfi_avg.index.values,\n",
    "     z_ctr+np.cumsum(dfi_avg['del_z']),\n",
    "    ]).T\n",
    "\n",
    "\n",
    "# Generate a new collection of streamline points with specificty density based on\n",
    "# the average trajectory of each interpolation at the center x, z value specified.\n",
    "\n",
    "fx_all = interpolate.interp1d(\n",
    "    all_pts[:, 1],\n",
    "    all_pts[:, 0],\n",
    "    kind=\"linear\",\n",
    "    fill_value=\"extrapolate\",\n",
    "    bounds_error=False,\n",
    ")\n",
    "\n",
    "fz_all = interpolate.interp1d(\n",
    "    all_pts[:, 1],\n",
    "    all_pts[:, 2],\n",
    "    kind=\"linear\",\n",
    "    fill_value=\"extrapolate\",\n",
    "    bounds_error=False,\n",
    ")\n",
    "\n",
    "\n",
    "new_y = np.arange(y_min, y_max, density)\n",
    "new_x = fx_all(new_y)\n",
    "new_z = fz_all(new_y)\n",
    "new_pts = np.vstack([new_x, new_y, new_z]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d166b0-f8e7-4769-99c5-aef5d7b90ade",
   "metadata": {},
   "source": [
    "### Sanity checking\n",
    "\n",
    "After producing your streamline, double check that it actually makes sense with neurons.\n",
    "The following will create a neuroglancer link showing the full streamline points.\n",
    "Look at the apical dendrites of deep layer cells and the primary axons of cells along cortical depth to make sure this matches expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c9b58e-f846-4c11-945b-bfb836ac220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, seg = statebuilder.from_client(client)\n",
    "\n",
    "ptmap = statebuilder.PointMapper()\n",
    "anno = statebuilder.AnnotationLayerConfig(\n",
    "    \"points\", array_data=True, mapping_rules=ptmap\n",
    ")\n",
    "\n",
    "sb = statebuilder.StateBuilder([img, seg, anno], client=client)\n",
    "\n",
    "sb.render_state(new_pts, return_as=\"html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e700d4f2-d3cb-4a96-bf03-830218935ffb",
   "metadata": {},
   "source": [
    "### Saving a streamline\n",
    "\n",
    "To generate a useful file for the Streamline class, the most consistent approach is to save it in the expected post-transform space that you will use when interacting with the Streamline class.\n",
    "Since we have done everything in voxels, the easiest thing to do is transform by the voxel transform.\n",
    "Here, we are in the Minnie65 data so we can use the prebaked transform there.\n",
    "In general, however, this might not be an existing function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef3801b-f6cd-45a2-9de4-702992c60bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'minnie_streamline_um.json'\n",
    "\n",
    "from standard_transform import minnie_transform_vx\n",
    "tform = minnie_transform_vx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc57c6-df21-4d9a-a161-caac5efd4547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "pts_tform = tform.apply(new_pts)\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(\n",
    "        pts_tform.tolist(),\n",
    "        f,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis_m1",
   "language": "python",
   "name": "analysis_m1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
